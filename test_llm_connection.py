#!/usr/bin/env python3
"""
å¤§æ¨¡å‹æœåŠ¡è¿æ¥æµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯Ollamaæˆ–OpenAIæ¨¡å‹æœåŠ¡æ˜¯å¦æ­£å¸¸å¯ç”¨
"""
import os
import sys
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))

from novel_generator.agents.base_agent import BaseAgent


def test_llm_connection():
    """æµ‹è¯•LLMè¿æ¥å’ŒåŸºæœ¬åŠŸèƒ½"""
    print("=" * 60)
    print("ğŸ” å¤§æ¨¡å‹æœåŠ¡è¿æ¥æµ‹è¯•")
    print("=" * 60)
    
    # æ˜¾ç¤ºå½“å‰é…ç½®ä¿¡æ¯
    print("\nğŸ“‹ å½“å‰ç¯å¢ƒé…ç½®:")
    print(f"  LLM_PROVIDER: {os.getenv('LLM_PROVIDER', 'æœªè®¾ç½®')}")
    print(f"  OPENAI_MODEL: {os.getenv('OPENAI_MODEL', 'æœªè®¾ç½®')}")
    print(f"  OPENAI_API_BASE: {os.getenv('OPENAI_API_BASE', 'æœªè®¾ç½®')}")
    print(f"  OLLAMA_MODEL: {os.getenv('OLLAMA_MODEL', 'æœªè®¾ç½®')}")
    print(f"  OLLAMA_BASE_URL: {os.getenv('OLLAMA_BASE_URL', 'æœªè®¾ç½®')}")
    print(f"  LLM_TEMPERATURE: {os.getenv('LLM_TEMPERATURE', 'æœªè®¾ç½®')}")
    
    # æ£€æŸ¥å¿…è¦çš„é…ç½®
    provider = os.getenv('LLM_PROVIDER', '').lower()
    if not provider:
        print("\nâŒ é”™è¯¯: LLM_PROVIDER ç¯å¢ƒå˜é‡æœªè®¾ç½®")
        print("  è¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½® LLM_PROVIDER=ollama æˆ– LLM_PROVIDER=openai")
        return False
    
    # æµ‹è¯•è¿æ¥
    print(f"\nğŸš€ å¼€å§‹æµ‹è¯• {provider.upper()} æ¨¡å‹è¿æ¥...")
    start_time = time.time()
    
    try:
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•æ™ºèƒ½ä½“
        class TestAgent(BaseAgent):
            def run(self, input_data):
                return {"result": "Test completed"}
        
        # åˆå§‹åŒ–æ™ºèƒ½ä½“ï¼ˆè¿™å°†åˆå§‹åŒ–LLMè¿æ¥ï¼‰
        test_agent = TestAgent(agent_name="TestAgent")
        print(f"âœ… æˆåŠŸåˆå§‹åŒ–æ™ºèƒ½ä½“å’ŒLLMè¿æ¥")
        
        # æµ‹è¯•åŸºæœ¬çš„LLMè°ƒç”¨åŠŸèƒ½
        print("\nğŸ’¬ æµ‹è¯•LLMæ–‡æœ¬ç”ŸæˆåŠŸèƒ½...")
        test_prompt = "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ï¼Œç”¨ä¸€å¥è¯å›ç­”ã€‚"
        response = test_agent.invoke_llm(test_prompt)
        
        if response and not response.startswith("âŒ"):
            print(f"âœ… LLMè°ƒç”¨æˆåŠŸ!")
            print(f"ğŸ“ å“åº”å†…å®¹: {response[:100]}..." if len(response) > 100 else f"ğŸ“ å“åº”å†…å®¹: {response}")
        else:
            print(f"âŒ LLMè°ƒç”¨å¤±è´¥: {response}")
            return False
        
        end_time = time.time()
        print(f"\nâ±ï¸  æµ‹è¯•å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f} ç§’")
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        
        # æä¾›é’ˆå¯¹æ€§çš„é”™è¯¯å»ºè®®
        if "Ollama" in str(e) or provider == "ollama":
            print("\nğŸ’¡ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
            print("  1. ç¡®ä¿OllamaæœåŠ¡å·²å¯åŠ¨: ollama serve")
            print("  2. ç¡®ä¿å·²å®‰è£…æŒ‡å®šçš„æ¨¡å‹: ollama pull", os.getenv("OLLAMA_MODEL", "qwen2.5:7b"))
            print("  3. æ£€æŸ¥OLLAMA_BASE_URLæ˜¯å¦æ­£ç¡®è®¾ç½®")
        
        elif provider == "openai":
            print("\nğŸ’¡ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
            print("  1. ç¡®ä¿OPENAI_API_KEYå·²æ­£ç¡®è®¾ç½®")
            print("  2. æ£€æŸ¥OPENAI_API_BASEæ˜¯å¦æ­£ç¡®é…ç½®")
            print("  3. ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸ä¸”èƒ½è®¿é—®APIæœåŠ¡")
        
        return False


if __name__ == "__main__":
    success = test_llm_connection()
    print("\n" + "=" * 60)
    if success:
        print("ğŸ‰ å¤§æ¨¡å‹æœåŠ¡è¿æ¥æµ‹è¯•æˆåŠŸ!")
    else:
        print("âŒ å¤§æ¨¡å‹æœåŠ¡è¿æ¥æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å’ŒæœåŠ¡çŠ¶æ€")
    print("=" * 60)
    sys.exit(0 if success else 1)