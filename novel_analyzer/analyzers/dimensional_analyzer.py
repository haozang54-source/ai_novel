"""
ç»´åº¦åˆ†æå™¨ - ç­–ç•¥2ï¼šåˆ†ç±»å¹¶è¡Œå¤„ç†
æŒ‰ä¸åŒç»´åº¦ï¼ˆè§’è‰²ã€æƒ…èŠ‚ã€ä¸–ç•Œè§‚ã€é£æ ¼ï¼‰åˆ†åˆ«å¤„ç†æ‰€æœ‰ç« èŠ‚æ•°æ®
"""
import json
import os
from pathlib import Path
from typing import Dict, List, Any, Optional
from collections import defaultdict


class DimensionalAnalyzer:
    """ç»´åº¦åˆ†æå™¨ - å¤šç»´åº¦å¹¶è¡Œåˆ†æ"""
    
    def __init__(self, llm, config: dict, aggregated_data: Dict[str, Any], output_dir: str):
        """
        åˆå§‹åŒ–ç»´åº¦åˆ†æå™¨
        
        Args:
            llm: LLMå®ä¾‹
            config: é…ç½®å­—å…¸
            aggregated_data: èšåˆæ•°æ®ï¼ˆæ¥è‡ªDataAggregatorï¼‰
            output_dir: è¾“å‡ºç›®å½•
        """
        self.llm = llm
        self.config = config
        self.data = aggregated_data
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # é…ç½®å‚æ•°
        self.retry_times = config.get('extraction', {}).get('retry_times', 3)
        self.timeout = config.get('extraction', {}).get('timeout', 120)
    
    def analyze_all_dimensions(self) -> Dict[str, Any]:
        """
        å¹¶è¡Œåˆ†ææ‰€æœ‰ç»´åº¦
        
        Returns:
            åŒ…å«æ‰€æœ‰ç»´åº¦åˆ†æç»“æœçš„å­—å…¸
        """
        print("\n" + "="*60)
        print("ğŸ¯ ç­–ç•¥2ï¼šå¤šç»´åº¦å¹¶è¡Œåˆ†æ")
        print("="*60)
        
        results = {}
        
        # ç»´åº¦1: è§’è‰²çº¿
        print("\nğŸ‘¥ ç»´åº¦1ï¼šè§’è‰²åˆ†æ...")
        results['character_dimension'] = self.analyze_character_dimension()
        
        # ç»´åº¦2: æƒ…èŠ‚çº¿
        print("\nğŸ“– ç»´åº¦2ï¼šæƒ…èŠ‚åˆ†æ...")
        results['plot_dimension'] = self.analyze_plot_dimension()
        
        # ç»´åº¦3: ä¸–ç•Œè§‚
        print("\nğŸŒ ç»´åº¦3ï¼šä¸–ç•Œè§‚åˆ†æ...")
        results['world_dimension'] = self.analyze_world_dimension()
        
        # ç»´åº¦4: é£æ ¼çº¿
        print("\nâœï¸  ç»´åº¦4ï¼šé£æ ¼åˆ†æ...")
        results['style_dimension'] = self.analyze_style_dimension()
        
        # ä¿å­˜ç»¼åˆç»“æœ
        output_file = self.output_dir / 'dimensional_analysis_complete.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ¨ å¤šç»´åº¦åˆ†æå®Œæˆï¼")
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        return results
    
    def analyze_character_dimension(self) -> Dict[str, Any]:
        """
        ç»´åº¦1ï¼šè§’è‰²çº¿åˆ†æ
        
        ç­–ç•¥ï¼š
        1. æ™ºèƒ½ç­›é€‰ï¼šåªä¿ç•™é‡è¦è§’è‰²ï¼ˆå‡ºåœº>5æ¬¡ï¼‰
        2. å…³ç³»å›¾è°±ï¼šæ„å»ºè§’è‰²å…³ç³»ç½‘ç»œ
        3. æˆé•¿è½¨è¿¹ï¼šè¿½è¸ªè§’è‰²å‘å±•
        """
        characters = self.data.get('characters', [])
        
        # ç­›é€‰é‡è¦è§’è‰²
        important_chars = [
            char for char in characters 
            if char.get('total_appearances', 0) > 5
        ]
        
        print(f"  åŸå§‹è§’è‰²æ•°: {len(characters)}")
        print(f"  é‡è¦è§’è‰²æ•°: {len(important_chars)} (å‡ºåœº>5æ¬¡)")
        
        # æ„å»ºè§’è‰²å…³ç³»ç½‘ç»œ
        relationships = self._extract_relationships(important_chars)
        
        # åˆ†ç±»è§’è‰²ï¼ˆä¸»è§’ã€é…è§’ã€åæ´¾ï¼‰
        categorized = self._categorize_characters(important_chars)
        
        # å‡†å¤‡ç»™LLMçš„å‹ç¼©æ•°æ®
        compressed_data = {
            'important_characters': [
                {
                    'name': char['name'],
                    'role': char['role'],
                    'appearances': char['total_appearances'],
                    'first_chapter': char['first_appearance_chapter'],
                    'traits': {
                        'appearance': char.get('appearance_traits', [])[:3],
                        'personality': char.get('personality_traits', [])[:3]
                    }
                }
                for char in important_chars[:20]  # åªä¿ç•™å‰20ä¸ªæœ€é‡è¦è§’è‰²
            ],
            'relationships': relationships,
            'categorization': categorized
        }
        
        # è°ƒç”¨LLMç”Ÿæˆè§’è‰²æ¨¡æ¿
        char_template = self._generate_character_template(compressed_data)
        
        # ä¿å­˜è§’è‰²ç»´åº¦åˆ†æç»“æœ
        result = {
            'total_characters': len(characters),
            'important_characters_count': len(important_chars),
            'compressed_data': compressed_data,
            'character_template': char_template,
            'metadata': {
                'compression_ratio': f"{len(important_chars)}/{len(characters)}",
                'kept_percentage': f"{len(important_chars)/len(characters)*100:.1f}%"
            }
        }
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        output_file = self.output_dir / 'dimension_1_characters.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        size_kb = output_file.stat().st_size / 1024
        print(f"  âœ… è§’è‰²åˆ†æå®Œæˆ: {size_kb:.2f} KB")
        print(f"  ğŸ“Š å‹ç¼©ç‡: {result['metadata']['compression_ratio']} ({result['metadata']['kept_percentage']})")
        
        return result
    
    def analyze_plot_dimension(self) -> Dict[str, Any]:
        """
        ç»´åº¦2ï¼šæƒ…èŠ‚çº¿åˆ†æ
        
        ç­–ç•¥ï¼š
        1. æ„å»ºæƒ…èŠ‚æ ‘ï¼šè¯†åˆ«ä¸»çº¿ã€æ”¯çº¿
        2. å…³é”®è½¬æŠ˜ï¼šæå–é‡è¦äº‹ä»¶ï¼ˆimportance=highï¼‰
        3. èŠ‚å¥åˆ†æï¼šç»Ÿè®¡æƒ…èŠ‚å¯†åº¦
        """
        events = self.data.get('events', [])
        plot_arcs = self.data.get('plot_arcs', [])
        
        # ç­›é€‰é‡è¦äº‹ä»¶
        important_events = [
            event for event in events 
            if event.get('importance') in ['high', 'critical']
        ]
        
        print(f"  æ€»äº‹ä»¶æ•°: {len(events)}")
        print(f"  é‡è¦äº‹ä»¶æ•°: {len(important_events)}")
        
        # æŒ‰ç« èŠ‚èŒƒå›´åˆ†æ®µåˆ†æï¼ˆæ¯50ç« ä¸€ä¸ªé‡Œç¨‹ç¢‘ï¼‰
        milestones = self._extract_plot_milestones(plot_arcs, important_events, segment_size=50)
        
        # è¯†åˆ«æƒ…èŠ‚ç±»å‹åˆ†å¸ƒ
        plot_types = self._categorize_plot_types(events)
        
        # å‡†å¤‡å‹ç¼©æ•°æ®
        compressed_data = {
            'milestones': milestones,
            'important_events': [
                {
                    'chapter': event['chapter_number'],
                    'type': event['type'],
                    'description': event['description'][:100],  # æˆªæ–­æè¿°
                    'participants': event.get('participants', [])[:5]
                }
                for event in important_events[:30]  # åªä¿ç•™å‰30ä¸ªé‡è¦äº‹ä»¶
            ],
            'plot_type_distribution': plot_types
        }
        
        # è°ƒç”¨LLMç”Ÿæˆæƒ…èŠ‚æ¡†æ¶
        plot_framework = self._generate_plot_framework(compressed_data)
        
        result = {
            'total_events': len(events),
            'important_events_count': len(important_events),
            'milestones_count': len(milestones),
            'compressed_data': compressed_data,
            'plot_framework': plot_framework,
            'metadata': {
                'compression_ratio': f"{len(important_events)}/{len(events)}",
                'kept_percentage': f"{len(important_events)/len(events)*100:.1f}%"
            }
        }
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        output_file = self.output_dir / 'dimension_2_plot.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        size_kb = output_file.stat().st_size / 1024
        print(f"  âœ… æƒ…èŠ‚åˆ†æå®Œæˆ: {size_kb:.2f} KB")
        print(f"  ğŸ“Š å‹ç¼©ç‡: {result['metadata']['compression_ratio']} ({result['metadata']['kept_percentage']})")
        
        return result
    
    def analyze_world_dimension(self) -> Dict[str, Any]:
        """
        ç»´åº¦3ï¼šä¸–ç•Œè§‚åˆ†æ
        
        ç­–ç•¥ï¼š
        1. å»é‡åˆå¹¶ï¼šç›¸åŒå…ƒç´ åªä¿ç•™ä¸€ä¸ª
        2. æŒ‰ç±»å‹åˆ†ç±»ï¼šåŠ›é‡ä½“ç³»ã€åœ°ç†ã€ç¤¾ä¼šç­‰
        3. æå–æ ¸å¿ƒè®¾å®š
        """
        world_elements = self.data.get('world_elements', {})
        locations = self.data.get('locations', [])
        
        # ç»Ÿè®¡å„ç±»å‹å…ƒç´ æ•°é‡
        element_stats = {
            elem_type: len(elements)
            for elem_type, elements in world_elements.items()
        }
        
        total_elements = sum(element_stats.values())
        print(f"  æ€»ä¸–ç•Œè§‚å…ƒç´ : {total_elements}")
        print(f"  å…ƒç´ ç±»å‹æ•°: {len(world_elements)}")
        
        # æ™ºèƒ½å»é‡å’Œå‹ç¼©
        compressed_world = {}
        for elem_type, elements in world_elements.items():
            # åªä¿ç•™å‰10ä¸ªæœ€æ—©å‡ºç°çš„å…ƒç´ 
            compressed_world[elem_type] = sorted(
                elements, 
                key=lambda x: x.get('first_mentioned_chapter', 999)
            )[:10]
        
        # ä¸»è¦åœ°ç‚¹ï¼ˆæŒ‰å‡ºåœºæ¬¡æ•°ï¼‰
        main_locations = sorted(
            locations,
            key=lambda x: len(x.get('appearance_chapters', [])),
            reverse=True
        )[:15]  # åªä¿ç•™å‰15ä¸ªåœ°ç‚¹
        
        compressed_data = {
            'world_elements_by_type': compressed_world,
            'main_locations': [
                {
                    'name': loc['name'],
                    'type': loc['type'],
                    'first_chapter': loc['first_appearance_chapter'],
                    'appearances': len(loc.get('appearance_chapters', [])),
                    'description': loc.get('descriptions', [{}])[0].get('description', '')[:100]
                }
                for loc in main_locations
            ],
            'element_statistics': element_stats
        }
        
        # è°ƒç”¨LLMç”Ÿæˆä¸–ç•Œè§‚è®¾å®š
        world_bible = self._generate_world_bible(compressed_data)
        
        result = {
            'total_elements': total_elements,
            'element_types': len(world_elements),
            'total_locations': len(locations),
            'compressed_data': compressed_data,
            'world_bible': world_bible,
            'metadata': {
                'compression_ratio': f"{sum(len(v) for v in compressed_world.values())}/{total_elements}",
            }
        }
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        output_file = self.output_dir / 'dimension_3_world.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        size_kb = output_file.stat().st_size / 1024
        print(f"  âœ… ä¸–ç•Œè§‚åˆ†æå®Œæˆ: {size_kb:.2f} KB")
        
        return result
    
    def analyze_style_dimension(self) -> Dict[str, Any]:
        """
        ç»´åº¦4ï¼šé£æ ¼åˆ†æ
        
        ç­–ç•¥ï¼š
        1. æ™ºèƒ½é‡‡æ ·ï¼šä¸éœ€è¦æ‰€æœ‰ç« èŠ‚ï¼Œé‡‡æ ·åˆ†æå³å¯
        2. ç»Ÿè®¡ç‰¹å¾ï¼šå™äº‹è§†è§’ã€æƒ…æ„Ÿå¼ºåº¦ã€æå†™é‡ç‚¹
        3. æå–å†™ä½œæ¨¡å¼
        """
        writing_styles = self.data.get('writing_styles', {})
        plot_arcs = self.data.get('plot_arcs', [])
        
        # é‡‡æ ·åˆ†æï¼ˆå¼€å¤´10ç«  + æ¯50ç« é‡‡æ ·5ç«  + ç»“å°¾10ç« ï¼‰
        sampled_chapters = self._sample_chapters_for_style(plot_arcs)
        
        print(f"  æ€»ç« èŠ‚æ•°: {len(plot_arcs)}")
        print(f"  é‡‡æ ·ç« èŠ‚æ•°: {len(sampled_chapters)}")
        
        # é£æ ¼ç»Ÿè®¡
        perspectives = writing_styles.get('narrative_perspectives', {})
        intensities = writing_styles.get('emotional_intensities', {})
        focuses = writing_styles.get('description_focuses', {})
        
        compressed_data = {
            'sampled_chapters': sampled_chapters,
            'narrative_style': {
                'perspectives': perspectives,
                'dominant_perspective': max(perspectives.items(), key=lambda x: x[1])[0] if perspectives else 'unknown'
            },
            'emotional_pattern': {
                'intensities': intensities,
                'average_intensity': self._calculate_avg_intensity(intensities)
            },
            'description_focus': focuses,
            'key_phrases_sample': writing_styles.get('key_phrases', [])[:20]
        }
        
        # è°ƒç”¨LLMç”Ÿæˆå†™ä½œæŒ‡å—
        writing_guide = self._generate_writing_guide(compressed_data)
        
        result = {
            'total_chapters': len(plot_arcs),
            'sampled_chapters_count': len(sampled_chapters),
            'compressed_data': compressed_data,
            'writing_guide': writing_guide,
            'metadata': {
                'sample_ratio': f"{len(sampled_chapters)}/{len(plot_arcs)}",
                'sample_percentage': f"{len(sampled_chapters)/len(plot_arcs)*100:.1f}%"
            }
        }
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        output_file = self.output_dir / 'dimension_4_style.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        size_kb = output_file.stat().st_size / 1024
        print(f"  âœ… é£æ ¼åˆ†æå®Œæˆ: {size_kb:.2f} KB")
        print(f"  ğŸ“Š é‡‡æ ·ç‡: {result['metadata']['sample_ratio']} ({result['metadata']['sample_percentage']})")
        
        return result
    
    # ========== è¾…åŠ©æ–¹æ³• ==========
    
    def _extract_relationships(self, characters: List[Dict]) -> Dict[str, List[str]]:
        """æå–è§’è‰²å…³ç³»ç½‘ç»œ"""
        relationships = defaultdict(list)
        
        for char in characters:
            char_name = char['name']
            for rel in char.get('relationships', []):
                if isinstance(rel, dict):
                    target = rel.get('target') or rel.get('with')
                    if target:
                        relationships[char_name].append(target)
        
        return dict(relationships)
    
    def _categorize_characters(self, characters: List[Dict]) -> Dict[str, List[str]]:
        """åˆ†ç±»è§’è‰²"""
        categorized = {
            'protagonist': [],
            'antagonist': [],
            'supporting': [],
            'minor': []
        }
        
        for char in characters:
            role = char.get('role', 'unknown')
            name = char['name']
            
            if 'protagonist' in role.lower() or 'main' in role.lower():
                categorized['protagonist'].append(name)
            elif 'antagonist' in role.lower() or 'villain' in role.lower():
                categorized['antagonist'].append(name)
            elif char.get('total_appearances', 0) > 10:
                categorized['supporting'].append(name)
            else:
                categorized['minor'].append(name)
        
        return categorized
    
    def _extract_plot_milestones(self, plot_arcs: List[Dict], 
                                  important_events: List[Dict],
                                  segment_size: int = 50) -> List[Dict]:
        """æå–æƒ…èŠ‚é‡Œç¨‹ç¢‘"""
        total_chapters = len(plot_arcs)
        milestones = []
        
        for i in range(0, total_chapters, segment_size):
            end_chapter = min(i + segment_size, total_chapters)
            
            # è¯¥æ®µçš„é‡è¦äº‹ä»¶
            segment_events = [
                e for e in important_events
                if i < e['chapter_number'] <= end_chapter
            ]
            
            if segment_events:
                milestones.append({
                    'chapter_range': f"{i+1}-{end_chapter}",
                    'key_events_count': len(segment_events),
                    'top_events': [e['description'][:50] for e in segment_events[:3]]
                })
        
        return milestones
    
    def _categorize_plot_types(self, events: List[Dict]) -> Dict[str, int]:
        """ç»Ÿè®¡æƒ…èŠ‚ç±»å‹åˆ†å¸ƒ"""
        type_counts = defaultdict(int)
        
        for event in events:
            event_type = event.get('type', 'unknown')
            type_counts[event_type] += 1
        
        return dict(sorted(type_counts.items(), key=lambda x: x[1], reverse=True))
    
    def _sample_chapters_for_style(self, plot_arcs: List[Dict]) -> List[Dict]:
        """æ™ºèƒ½é‡‡æ ·ç« èŠ‚ç”¨äºé£æ ¼åˆ†æ"""
        total = len(plot_arcs)
        sampled = []
        
        # å¼€å¤´10ç« 
        sampled.extend(plot_arcs[:10])
        
        # æ¯50ç« é‡‡æ ·5ç« 
        for i in range(10, total - 10, 50):
            sampled.extend(plot_arcs[i:min(i+5, total-10)])
        
        # ç»“å°¾10ç« 
        sampled.extend(plot_arcs[-10:])
        
        return [
            {
                'chapter': arc['chapter_number'],
                'title': arc['chapter_title'],
                'word_count': arc['word_count']
            }
            for arc in sampled
        ]
    
    def _calculate_avg_intensity(self, intensities: Dict[str, int]) -> str:
        """è®¡ç®—å¹³å‡æƒ…æ„Ÿå¼ºåº¦"""
        intensity_scores = {
            'low': 1,
            'medium': 2,
            'high': 3,
            'very_high': 4
        }
        
        total_score = 0
        total_count = 0
        
        for intensity, count in intensities.items():
            score = intensity_scores.get(intensity.lower(), 2)
            total_score += score * count
            total_count += count
        
        if total_count == 0:
            return 'medium'
        
        avg_score = total_score / total_count
        
        if avg_score < 1.5:
            return 'low'
        elif avg_score < 2.5:
            return 'medium'
        elif avg_score < 3.5:
            return 'high'
        else:
            return 'very_high'
    
    # ========== LLMè°ƒç”¨æ–¹æ³•ï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦å®Œå–„promptï¼‰==========
    
    def _generate_character_template(self, data: Dict) -> Dict:
        """ç”Ÿæˆè§’è‰²æ¨¡æ¿ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # TODO: è°ƒç”¨LLMç”Ÿæˆè§’è‰²æ¨¡æ¿
        # è¿™é‡Œå…ˆè¿”å›å‹ç¼©åçš„æ•°æ®ç»“æ„ä½œä¸ºdemo
        return {
            'template_type': 'character_template',
            'main_characters': data['important_characters'][:5],
            'relationships': data['relationships'],
            'note': 'è¿™æ˜¯ç®€åŒ–ç‰ˆdemoï¼Œå®é™…ä½¿ç”¨æ—¶ä¼šè°ƒç”¨LLMç”Ÿæˆè¯¦ç»†æ¨¡æ¿'
        }
    
    def _generate_plot_framework(self, data: Dict) -> Dict:
        """ç”Ÿæˆæƒ…èŠ‚æ¡†æ¶ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        return {
            'template_type': 'plot_framework',
            'milestones': data['milestones'],
            'key_events': data['important_events'][:10],
            'note': 'è¿™æ˜¯ç®€åŒ–ç‰ˆdemoï¼Œå®é™…ä½¿ç”¨æ—¶ä¼šè°ƒç”¨LLMç”Ÿæˆè¯¦ç»†æ¡†æ¶'
        }
    
    def _generate_world_bible(self, data: Dict) -> Dict:
        """ç”Ÿæˆä¸–ç•Œè§‚è®¾å®šï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        return {
            'template_type': 'world_bible',
            'world_elements': data['world_elements_by_type'],
            'main_locations': data['main_locations'][:5],
            'note': 'è¿™æ˜¯ç®€åŒ–ç‰ˆdemoï¼Œå®é™…ä½¿ç”¨æ—¶ä¼šè°ƒç”¨LLMç”Ÿæˆè¯¦ç»†è®¾å®š'
        }
    
    def _generate_writing_guide(self, data: Dict) -> Dict:
        """ç”Ÿæˆå†™ä½œæŒ‡å—ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        return {
            'template_type': 'writing_guide',
            'narrative_style': data['narrative_style'],
            'emotional_pattern': data['emotional_pattern'],
            'key_phrases': data['key_phrases_sample'][:10],
            'note': 'è¿™æ˜¯ç®€åŒ–ç‰ˆdemoï¼Œå®é™…ä½¿ç”¨æ—¶ä¼šè°ƒç”¨LLMç”Ÿæˆè¯¦ç»†æŒ‡å—'
        }
