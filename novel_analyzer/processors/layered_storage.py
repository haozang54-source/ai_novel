"""
åˆ†å±‚å­˜å‚¨ç”Ÿæˆå™¨ - åˆ›å»ºAIå‹å¥½çš„å¤šå±‚å­˜å‚¨ç»“æ„
"""
import json
import os
from pathlib import Path
from typing import Dict, List, Any, Optional
from .data_aggregator import DataAggregator
from novel_analyzer.utils.smart_chunker import SmartChunker


class LayeredStorageGenerator:
    """åˆ†å±‚å­˜å‚¨ç”Ÿæˆå™¨ï¼Œåˆ›å»ºraw/aggregated/chunked/indexes/rag_readyäº”å±‚ç»“æ„"""
    
    def __init__(self, novel_name: str, base_output_dir: str, model_type: str = 'gpt4'):
        """
        åˆå§‹åŒ–åˆ†å±‚å­˜å‚¨ç”Ÿæˆå™¨
        
        Args:
            novel_name: å°è¯´åç§°
            base_output_dir: åŸºç¡€è¾“å‡ºç›®å½•
            model_type: ç›®æ ‡LLMç±»å‹ï¼ˆç”¨äºåˆ†å—å¤§å°ï¼‰
        """
        self.novel_name = novel_name
        self.base_path = Path(base_output_dir) / novel_name
        self.model_type = model_type
        self.chunker = SmartChunker(model_type=model_type)
        
        # å®šä¹‰å„å±‚ç›®å½•
        self.layers = {
            'raw': self.base_path / 'raw',
            'aggregated': self.base_path / 'aggregated',
            'chunked': self.base_path / 'chunked',
            'indexes': self.base_path / 'indexes',
            'rag_ready': self.base_path / 'rag_ready'
        }
        
        # åˆ›å»ºç›®å½•
        for path in self.layers.values():
            path.mkdir(parents=True, exist_ok=True)
    
    def generate_all_layers(self, chapter_summaries_dir: str):
        """
        ç”Ÿæˆæ‰€æœ‰å±‚çº§çš„å­˜å‚¨ç»“æ„
        
        Args:
            chapter_summaries_dir: ç« èŠ‚æ‘˜è¦ç›®å½•
        """
        print(f"ğŸ—ï¸  å¼€å§‹ç”Ÿæˆåˆ†å±‚å­˜å‚¨ç»“æ„: {self.novel_name}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.base_path}")
        print(f"ğŸ¤– ç›®æ ‡æ¨¡å‹: {self.model_type} (æœ€å¤§å—: {self.chunker.max_size/1024:.0f}KB)\n")
        
        # åˆ›å»ºèšåˆå™¨
        aggregator = DataAggregator(chapter_summaries_dir)
        aggregated_data = aggregator.create_aggregated_data()
        
        # Layer 1: Raw - ä¿å­˜åŸå§‹å®Œæ•´æ•°æ®
        print("\nğŸ“¦ Layer 1: ç”Ÿæˆ Raw å±‚...")
        self._generate_raw_layer(aggregated_data)
        
        # Layer 2: Aggregated - ä¿å­˜åˆ†ç±»èšåˆæ•°æ®
        print("\nğŸ“Š Layer 2: ç”Ÿæˆ Aggregated å±‚...")
        self._generate_aggregated_layer(aggregated_data)
        
        # Layer 3: Chunked - ç”ŸæˆAIå‹å¥½åˆ†å—
        print("\nâœ‚ï¸  Layer 3: ç”Ÿæˆ Chunked å±‚...")
        self._generate_chunked_layer(aggregated_data)
        
        # Layer 4: Indexes - ç”Ÿæˆå¿«é€Ÿç´¢å¼•
        print("\nğŸ—‚ï¸  Layer 4: ç”Ÿæˆ Indexes å±‚...")
        self._generate_indexes_layer(aggregated_data)
        
        # Layer 5: RAG Ready - ç”Ÿæˆå‘é‡æ£€ç´¢æ ¼å¼
        print("\nğŸ” Layer 5: ç”Ÿæˆ RAG Ready å±‚...")
        self._generate_rag_layer(aggregated_data)
        
        print(f"\nâœ¨ åˆ†å±‚å­˜å‚¨ç”Ÿæˆå®Œæˆï¼")
        self._print_storage_summary()
    
    def _generate_raw_layer(self, data: Dict[str, Any]):
        """Layer 1: åŸå§‹å®Œæ•´æ•°æ®ï¼ˆå•æ–‡ä»¶ï¼‰"""
        raw_file = self.layers['raw'] / f"{self.novel_name}_complete.json"
        
        with open(raw_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        size_kb = raw_file.stat().st_size / 1024
        print(f"  âœ… å®Œæ•´æ•°æ®: {size_kb:.2f} KB")
    
    def _generate_aggregated_layer(self, data: Dict[str, Any]):
        """Layer 2: åˆ†ç±»èšåˆæ•°æ®ï¼ˆæŒ‰ç±»åˆ«åˆ†æ–‡ä»¶ï¼‰"""
        aggregated_dir = self.layers['aggregated']
        
        categories = {
            'characters.json': data['characters'],
            'locations.json': data['locations'],
            'events.json': data['events'],
            'world_elements.json': data['world_elements'],
            'writing_styles.json': data['writing_styles'],
            'plot_arcs.json': data['plot_arcs'],
            'metadata.json': data['metadata']
        }
        
        for filename, content in categories.items():
            file_path = aggregated_dir / filename
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(content, f, ensure_ascii=False, indent=2)
            
            size_kb = file_path.stat().st_size / 1024
            print(f"  âœ… {filename}: {size_kb:.2f} KB")
    
    def _generate_chunked_layer(self, data: Dict[str, Any]):
        """Layer 3: AIå‹å¥½åˆ†å—ï¼ˆæŒ‰å¤§å°å’Œè¯­ä¹‰åˆ†å—ï¼‰"""
        chunked_dir = self.layers['chunked']
        
        # Characters - æŒ‰è§’è‰²é‡è¦æ€§ï¼ˆå‡ºåœºæ¬¡æ•°ï¼‰åˆ†å—
        self._chunk_and_save(
            data['characters'],
            chunked_dir / 'characters',
            'characters',
            group_by='role'
        )
        
        # Locations - æŒ‰åœ°ç‚¹ç±»å‹åˆ†å—
        self._chunk_and_save(
            data['locations'],
            chunked_dir / 'locations',
            'locations',
            group_by='type'
        )
        
        # Events - æŒ‰äº‹ä»¶ç±»å‹åˆ†å—
        self._chunk_and_save(
            data['events'],
            chunked_dir / 'events',
            'events',
            group_by='type'
        )
        
        # World Elements - å·²æŒ‰ç±»å‹åˆ†ç±»ï¼Œæ¯ä¸ªç±»å‹å•ç‹¬åˆ†å—
        world_dir = chunked_dir / 'world_elements'
        world_dir.mkdir(parents=True, exist_ok=True)
        
        for elem_type, elements in data['world_elements'].items():
            safe_type = elem_type.replace('/', '_')
            self._chunk_and_save(
                elements,
                world_dir,
                f"{safe_type}",
                group_by=None
            )
        
        # Plot Arcs - æŒ‰ç« èŠ‚èŒƒå›´åˆ†å—ï¼ˆæ¯20ç« ä¸€å—ï¼‰
        plot_chunks = self.chunker.chunk_by_chapters(
            data['plot_arcs'],
            chapters_per_chunk=20
        )
        
        plot_dir = chunked_dir / 'plot_arcs'
        plot_dir.mkdir(parents=True, exist_ok=True)
        
        for i, chunk in enumerate(plot_chunks):
            start_ch = chunk[0]['chapter_number']
            end_ch = chunk[-1]['chapter_number']
            file_path = plot_dir / f"chapters_{start_ch:03d}-{end_ch:03d}.json"
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(chunk, f, ensure_ascii=False, indent=2)
            
            info = self.chunker.get_chunk_info(chunk)
            print(f"  âœ… ç¬¬{start_ch}-{end_ch}ç« : {info['size_kb']} KB, {info['item_count']}ç« ")
    
    def _generate_indexes_layer(self, data: Dict[str, Any]):
        """Layer 4: å¿«é€Ÿç´¢å¼•ï¼ˆè½»é‡çº§æŸ¥æ‰¾è¡¨ï¼‰"""
        indexes_dir = self.layers['indexes']
        
        # è§’è‰²ç´¢å¼•
        char_index = {
            char['name']: {
                'role': char['role'],
                'first_chapter': char['first_appearance_chapter'],
                'total_appearances': char['total_appearances'],
                'appearance_chapters': [c['chapter_number'] for c in char['appearance_chapters']]
            }
            for char in data['characters']
        }
        
        self._save_index(indexes_dir / 'character_index.json', char_index)
        
        # åœ°ç‚¹ç´¢å¼•
        loc_index = {
            loc['name']: {
                'type': loc['type'],
                'first_chapter': loc['first_appearance_chapter'],
                'appearance_chapters': [c['chapter_number'] for c in loc['appearance_chapters']]
            }
            for loc in data['locations']
        }
        
        self._save_index(indexes_dir / 'location_index.json', loc_index)
        
        # ç« èŠ‚ç´¢å¼•
        chapter_index = {
            arc['chapter_number']: {
                'title': arc['chapter_title'],
                'word_count': arc['word_count'],
                'key_points_count': len(arc['key_points'])
            }
            for arc in data['plot_arcs']
        }
        
        self._save_index(indexes_dir / 'chapter_index.json', chapter_index)
        
        # ä¸–ç•Œè§‚å…ƒç´ ç´¢å¼•
        world_index = {
            elem_type: [e['element'] for e in elements]
            for elem_type, elements in data['world_elements'].items()
        }
        
        self._save_index(indexes_dir / 'world_elements_index.json', world_index)
    
    def _generate_rag_layer(self, data: Dict[str, Any]):
        """Layer 5: RAGæ£€ç´¢æ ¼å¼ï¼ˆJSONLæ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªå¯æ£€ç´¢å•å…ƒï¼‰"""
        rag_dir = self.layers['rag_ready']
        
        # Characters RAG
        char_rag_path = rag_dir / 'characters.jsonl'
        with open(char_rag_path, 'w', encoding='utf-8') as f:
            for char in data['characters']:
                rag_item = {
                    'id': f"char_{char['name']}",
                    'type': 'character',
                    'name': char['name'],
                    'content': self._create_character_text(char),
                    'metadata': {
                        'role': char['role'],
                        'first_chapter': char['first_appearance_chapter'],
                        'total_appearances': char['total_appearances']
                    }
                }
                f.write(json.dumps(rag_item, ensure_ascii=False) + '\n')
        
        print(f"  âœ… characters.jsonl: {len(data['characters'])} æ¡")
        
        # Locations RAG
        loc_rag_path = rag_dir / 'locations.jsonl'
        with open(loc_rag_path, 'w', encoding='utf-8') as f:
            for loc in data['locations']:
                rag_item = {
                    'id': f"loc_{loc['name']}",
                    'type': 'location',
                    'name': loc['name'],
                    'content': self._create_location_text(loc),
                    'metadata': {
                        'type': loc['type'],
                        'first_chapter': loc['first_appearance_chapter']
                    }
                }
                f.write(json.dumps(rag_item, ensure_ascii=False) + '\n')
        
        print(f"  âœ… locations.jsonl: {len(data['locations'])} æ¡")
        
        # Events RAG
        events_rag_path = rag_dir / 'events.jsonl'
        with open(events_rag_path, 'w', encoding='utf-8') as f:
            for i, event in enumerate(data['events']):
                rag_item = {
                    'id': f"event_{event['chapter_number']}_{i}",
                    'type': 'event',
                    'content': event['description'],
                    'metadata': {
                        'chapter': event['chapter_number'],
                        'event_type': event['type'],
                        'importance': event['importance'],
                        'participants': event['participants']
                    }
                }
                f.write(json.dumps(rag_item, ensure_ascii=False) + '\n')
        
        print(f"  âœ… events.jsonl: {len(data['events'])} æ¡")
        
        # Plot Arcs RAG
        plot_rag_path = rag_dir / 'plot_arcs.jsonl'
        with open(plot_rag_path, 'w', encoding='utf-8') as f:
            for arc in data['plot_arcs']:
                rag_item = {
                    'id': f"chapter_{arc['chapter_number']}",
                    'type': 'plot_arc',
                    'content': self._create_plot_text(arc),
                    'metadata': {
                        'chapter': arc['chapter_number'],
                        'title': arc['chapter_title'],
                        'word_count': arc['word_count']
                    }
                }
                f.write(json.dumps(rag_item, ensure_ascii=False) + '\n')
        
        print(f"  âœ… plot_arcs.jsonl: {len(data['plot_arcs'])} æ¡")
    
    def _chunk_and_save(self, items: List[Dict], output_dir: Path, 
                        category: str, group_by: Optional[str] = None):
        """åˆ†å—å¹¶ä¿å­˜æ•°æ®"""
        output_dir.mkdir(parents=True, exist_ok=True)
        
        chunks = self.chunker.chunk_by_items(items, group_key=group_by)
        
        for i, chunk in enumerate(chunks):
            file_path = output_dir / f"{category}_part_{i+1:02d}.json"
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(chunk, f, ensure_ascii=False, indent=2)
            
            info = self.chunker.get_chunk_info(chunk)
            print(f"  âœ… {category}_part_{i+1:02d}: {info['size_kb']} KB, {info['item_count']}é¡¹, {info['utilization']:.1f}%åˆ©ç”¨ç‡")
    
    def _save_index(self, file_path: Path, index_data: Dict):
        """ä¿å­˜ç´¢å¼•æ–‡ä»¶"""
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(index_data, f, ensure_ascii=False, indent=2)
        
        size_kb = file_path.stat().st_size / 1024
        print(f"  âœ… {file_path.name}: {size_kb:.2f} KB")
    
    def _create_character_text(self, char: Dict) -> str:
        """ä¸ºè§’è‰²åˆ›å»ºRAGæ£€ç´¢æ–‡æœ¬"""
        parts = [
            f"è§’è‰²åç§°ï¼š{char['name']}",
            f"è§’è‰²å®šä½ï¼š{char['role']}",
            f"é¦–æ¬¡å‡ºåœºï¼šç¬¬{char['first_appearance_chapter']}ç« ",
            f"æ€»å‡ºåœºæ¬¡æ•°ï¼š{char['total_appearances']}æ¬¡"
        ]
        
        if char.get('appearance_traits'):
            parts.append(f"å¤–è²Œç‰¹å¾ï¼š{', '.join(char['appearance_traits'])}")
        
        if char.get('personality_traits'):
            parts.append(f"æ€§æ ¼ç‰¹å¾ï¼š{', '.join(char['personality_traits'])}")
        
        if char.get('status_changes'):
            changes = [f"ç¬¬{s['chapter']}ç« ï¼š{s['change']}" for s in char['status_changes'][:5]]
            parts.append(f"å…³é”®è½¬å˜ï¼š{'; '.join(changes)}")
        
        return '\n'.join(parts)
    
    def _create_location_text(self, loc: Dict) -> str:
        """ä¸ºåœ°ç‚¹åˆ›å»ºRAGæ£€ç´¢æ–‡æœ¬"""
        parts = [
            f"åœ°ç‚¹åç§°ï¼š{loc['name']}",
            f"åœ°ç‚¹ç±»å‹ï¼š{loc['type']}",
            f"é¦–æ¬¡å‡ºç°ï¼šç¬¬{loc['first_appearance_chapter']}ç« "
        ]
        
        if loc.get('descriptions'):
            desc = loc['descriptions'][0]['description']
            parts.append(f"æè¿°ï¼š{desc}")
        
        return '\n'.join(parts)
    
    def _create_plot_text(self, arc: Dict) -> str:
        """ä¸ºæƒ…èŠ‚åˆ›å»ºRAGæ£€ç´¢æ–‡æœ¬"""
        parts = [
            f"ç« èŠ‚ï¼šç¬¬{arc['chapter_number']}ç«  {arc['chapter_title']}",
            f"ä¸»è¦å†…å®¹ï¼š{arc['main_content']}"
        ]
        
        if arc.get('key_points'):
            parts.append(f"å…³é”®è¦ç‚¹ï¼š{'; '.join(arc['key_points'])}")
        
        if arc.get('chapter_purpose'):
            parts.append(f"ç« èŠ‚ä½œç”¨ï¼š{arc['chapter_purpose']}")
        
        return '\n'.join(parts)
    
    def _print_storage_summary(self):
        """æ‰“å°å­˜å‚¨ç»“æ„æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š å­˜å‚¨ç»“æ„æ‘˜è¦")
        print("="*60)
        
        for layer_name, layer_path in self.layers.items():
            total_size = sum(f.stat().st_size for f in layer_path.rglob('*') if f.is_file())
            file_count = len(list(layer_path.rglob('*.json*')))
            
            print(f"\n{layer_name.upper()} å±‚:")
            print(f"  ğŸ“ è·¯å¾„: {layer_path}")
            print(f"  ğŸ“„ æ–‡ä»¶æ•°: {file_count}")
            print(f"  ğŸ’¾ æ€»å¤§å°: {total_size/1024:.2f} KB")
        
        print("\n" + "="*60)
