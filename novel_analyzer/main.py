"""
çˆ†æ¬¾åˆ†æå·¥ä½œæµä¸»ç¨‹åº
"""
import os
import sys
import yaml
import argparse
import time
from datetime import datetime
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# å¯¼å…¥LLMç›¸å…³åŒ…
try:
    from langchain_ollama import OllamaLLM
except ImportError:
    from langchain_community.llms import Ollama as OllamaLLM

try:
    from langchain_openai import ChatOpenAI
except ImportError:
    from langchain_community.chat_models import ChatOpenAI

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from analyzers.preprocessor import NovelPreprocessor
from analyzers.chapter_analyzer import ChapterAnalyzer
from analyzers.segment_summarizer import SegmentSummarizer
from analyzers.global_analyzer import GlobalAnalyzer
from analyzers.template_generator import TemplateGenerator


def load_config(config_path: str = None) -> dict:
    """
    åŠ è½½é…ç½®æ–‡ä»¶
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        é…ç½®å­—å…¸
    """
    if config_path is None:
        config_path = os.path.join(os.path.dirname(__file__), 'config', 'config.yaml')
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    return config


def init_llm(config: dict):
    """
    åˆå§‹åŒ–LLMï¼ˆä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ï¼‰
    
    Args:
        config: é…ç½®å­—å…¸
        
    Returns:
        LLMå®ä¾‹
    """
    llm_config = config.get('llm', {})
    
    # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®ï¼ˆä¼˜å…ˆçº§é«˜äºconfig.yamlï¼‰
    provider = os.getenv('LLM_PROVIDER', llm_config.get('provider', 'ollama'))
    temperature = float(os.getenv('LLM_TEMPERATURE', llm_config.get('temperature', 0.3)))
    max_tokens = int(os.getenv('LLM_MAX_TOKENS', llm_config.get('max_tokens', 3000)))
    
    if provider == 'ollama':
        model = os.getenv('OLLAMA_MODEL', llm_config.get('model', 'qwen2.5:7b-instruct'))
        base_url = os.getenv('OLLAMA_BASE_URL', llm_config.get('base_url', 'http://localhost:11434'))
        
        llm = OllamaLLM(
            model=model,
            base_url=base_url,
            temperature=temperature,
        )
        print(f"âœ“ ä½¿ç”¨ Ollama æ¨¡å‹")
        print(f"  æ¨¡å‹: {model}")
        print(f"  åœ°å€: {base_url}")
    
    elif provider == 'openai':
        model = os.getenv('OPENAI_MODEL', llm_config.get('model', 'gpt-3.5-turbo'))
        base_url = os.getenv('OPENAI_API_BASE', llm_config.get('base_url'))
        api_key = os.getenv('OPENAI_API_KEY', llm_config.get('api_key', 'dummy'))
        
        llm = ChatOpenAI(
            model=model,
            base_url=base_url,
            api_key=api_key,
            temperature=temperature,
            max_tokens=max_tokens,
        )
        print(f"âœ“ ä½¿ç”¨ OpenAI å…¼å®¹æ¥å£")
        print(f"  æ¨¡å‹: {model}")
        print(f"  åœ°å€: {base_url}")
        print(f"  æ¸©åº¦: {temperature}, æœ€å¤§Token: {max_tokens}")
    
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„LLM provider: {provider}")
    
    return llm


def check_time_allowed(config: dict) -> bool:
    """
    æ£€æŸ¥å½“å‰æ—¶é—´æ˜¯å¦åœ¨å…è®¸çš„è¿è¡Œæ—¶é—´æ®µå†…
    
    Args:
        config: é…ç½®å­—å…¸
        
    Returns:
        æ˜¯å¦å…è®¸è¿è¡Œ
    """
    runtime_config = config.get('runtime', {})
    if not runtime_config:
        return True  # å¦‚æœæ²¡æœ‰é…ç½®è¿è¡Œæ—¶é—´é™åˆ¶ï¼Œé»˜è®¤å…è®¸
    
    start_hour = runtime_config.get('start', 22)  # é»˜è®¤æ™šä¸Š10ç‚¹
    end_hour = runtime_config.get('end', 8)       # é»˜è®¤æ—©ä¸Š8ç‚¹
    
    current_hour = datetime.now().hour
    
    # å¤„ç†è·¨å¤©çš„æƒ…å†µï¼ˆå¦‚ 22ç‚¹åˆ°æ¬¡æ—¥8ç‚¹ï¼‰
    if start_hour > end_hour:
        # è·¨å¤©ï¼š22ç‚¹-24ç‚¹ æˆ– 0ç‚¹-8ç‚¹
        allowed = current_hour >= start_hour or current_hour < end_hour
    else:
        # ä¸è·¨å¤©ï¼š8ç‚¹-22ç‚¹
        allowed = start_hour <= current_hour < end_hour
    
    return allowed


def wait_for_allowed_time(config: dict):
    """
    ç­‰å¾…åˆ°å…è®¸çš„è¿è¡Œæ—¶é—´æ®µ
    
    Args:
        config: é…ç½®å­—å…¸
    """
    runtime_config = config.get('runtime', {})
    if not runtime_config:
        return  # å¦‚æœæ²¡æœ‰é…ç½®è¿è¡Œæ—¶é—´é™åˆ¶ï¼Œç›´æ¥è¿”å›
    
    start_hour = runtime_config.get('start', 22)
    end_hour = runtime_config.get('end', 8)
    
    while not check_time_allowed(config):
        current_time = datetime.now()
        current_hour = current_time.hour
        
        # è®¡ç®—è·ç¦»ä¸‹æ¬¡å…è®¸æ—¶é—´çš„å°æ—¶æ•°
        if start_hour > end_hour:  # è·¨å¤©
            if current_hour < start_hour and current_hour >= end_hour:
                hours_to_wait = start_hour - current_hour
            else:
                hours_to_wait = 1
        else:  # ä¸è·¨å¤©
            hours_to_wait = start_hour - current_hour if current_hour < start_hour else 24 - current_hour + start_hour
        
        print(f"\nâ° å½“å‰æ—¶é—´ {current_time.strftime('%H:%M')} ä¸åœ¨å…è®¸çš„è¿è¡Œæ—¶é—´æ®µå†…")
        print(f"   å…è®¸è¿è¡Œæ—¶é—´ï¼š{start_hour:02d}:00 - {end_hour:02d}:00")
        print(f"   é¢„è®¡ç­‰å¾…çº¦ {hours_to_wait} å°æ—¶")
        print(f"   å°†æ¯5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡...\n")
        
        time.sleep(300)  # ç­‰å¾…5åˆ†é’Ÿåé‡æ–°æ£€æŸ¥


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='çˆ†æ¬¾å°è¯´åˆ†æå·¥å…·')
    parser.add_argument('--input', '-i', required=True, help='å°è¯´æ–‡ä»¶å¤¹è·¯å¾„')
    parser.add_argument('--output', '-o', required=True, help='è¾“å‡ºæ¨¡æ¿ç›®å½•')
    parser.add_argument('--config', '-c', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--no-time-check', action='store_true', help='è·³è¿‡è¿è¡Œæ—¶é—´æ£€æŸ¥')
    parser.add_argument('--use-v2', action='store_true', help='ä½¿ç”¨V2åˆ†æ®µè¾“å‡ºç‰ˆæœ¬ï¼ˆæ›´ç¨³å®šï¼Œå®¹é”™æ€§æ›´å¼ºï¼‰')
    parser.add_argument('--aggregate', action='store_true', help='èšåˆç« èŠ‚æ•°æ®å¹¶ç”Ÿæˆåˆ†å±‚å­˜å‚¨')
    parser.add_argument('--model-type', default='gpt4', choices=['gpt4', 'claude', 'llama3'],
                       help='ç›®æ ‡LLMç±»å‹ï¼ˆç”¨äºåˆ†å—å¤§å°æ§åˆ¶ï¼‰')
    
    args = parser.parse_args()
    
    # æ‰“å°æ¬¢è¿ä¿¡æ¯
    print("\n" + "="*60)
    print("ğŸ“š çˆ†æ¬¾å°è¯´åˆ†æå·¥å…·")
    print("="*60 + "\n")
    
    start_time = datetime.now()
    
    # åŠ è½½é…ç½®
    print("âš™ï¸  åŠ è½½é…ç½®...")
    config = load_config(args.config)
    
    # æ£€æŸ¥è¿è¡Œæ—¶é—´ï¼ˆé™¤éä½¿ç”¨äº† --no-time-check å‚æ•°ï¼‰
    if not args.no_time_check:
        print("ğŸ• æ£€æŸ¥è¿è¡Œæ—¶é—´...")
        wait_for_allowed_time(config)
        
        if check_time_allowed(config):
            runtime_config = config.get('runtime', {})
            if runtime_config:
                start = runtime_config.get('start', 22)
                end = runtime_config.get('end', 8)
                print(f"âœ“ å½“å‰æ—¶é—´å…è®¸è¿è¡Œï¼ˆå…è®¸æ—¶æ®µï¼š{start:02d}:00-{end:02d}:00ï¼‰")
    else:
        print("âš ï¸  å·²è·³è¿‡è¿è¡Œæ—¶é—´æ£€æŸ¥")
    
    # åˆå§‹åŒ–LLM
    print("ğŸ¤– åˆå§‹åŒ–LLM...")
    llm = init_llm(config)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output, exist_ok=True)
    intermediate_dir = os.path.join(args.output, 'intermediate')
    os.makedirs(intermediate_dir, exist_ok=True)
    
    try:
        # å¦‚æœåªæ˜¯èšåˆæ•°æ®ï¼Œè·³è¿‡åˆ†ææµç¨‹
        if args.aggregate:
            from processors.layered_storage import LayeredStorageGenerator
            
            # æå–å°è¯´åç§°
            novel_name = os.path.basename(args.input.rstrip('/'))
            chapter_summaries_dir = os.path.join(args.output, 'intermediate', 'chapter_summaries')
            
            # æ£€æŸ¥ç« èŠ‚æ‘˜è¦ç›®å½•æ˜¯å¦å­˜åœ¨
            if not os.path.exists(chapter_summaries_dir):
                print(f"âŒ ç« èŠ‚æ‘˜è¦ç›®å½•ä¸å­˜åœ¨: {chapter_summaries_dir}")
                print("   è¯·å…ˆè¿è¡Œç« èŠ‚åˆ†æç”Ÿæˆæ‘˜è¦æ•°æ®")
                return
            
            # åˆ›å»ºåˆ†å±‚å­˜å‚¨ç”Ÿæˆå™¨
            storage_dir = os.path.join(args.output, 'knowledge_base')
            generator = LayeredStorageGenerator(novel_name, storage_dir, args.model_type)
            
            # ç”Ÿæˆæ‰€æœ‰å±‚çº§
            generator.generate_all_layers(chapter_summaries_dir)
            return
        
        # ç¬¬ä¸€æ­¥ï¼šé¢„å¤„ç†
        print("\n" + "="*60)
        print("æ­¥éª¤ 1: æ–‡ä»¶é¢„å¤„ç†")
        print("="*60)
        preprocessor = NovelPreprocessor(args.input, config)
        chapters = preprocessor.load_and_process()
        
        if not chapters:
            print("âŒ æ²¡æœ‰å¯å¤„ç†çš„ç« èŠ‚ï¼Œé€€å‡º")
            return
        
        # ç¬¬äºŒæ­¥ï¼šå•ç« åˆ†æ
        print("\n" + "="*60)
        print("æ­¥éª¤ 2: å•ç« åˆ†æ")
        print("="*60)
        
        # å†æ¬¡æ£€æŸ¥æ—¶é—´ï¼ˆåˆ†æå¯èƒ½å¾ˆé•¿ï¼‰
        if not args.no_time_check and not check_time_allowed(config):
            print("âš ï¸  å·²è¶…å‡ºå…è®¸çš„è¿è¡Œæ—¶é—´æ®µï¼Œæš‚åœåˆ†æ...")
            wait_for_allowed_time(config)
            print("âœ“ æ¢å¤åˆ†æ...")
        
        # æ ¹æ®å‚æ•°é€‰æ‹©åˆ†æå™¨ç‰ˆæœ¬
        if args.use_v2:
            from analyzers.chapter_analyzer_v2 import ChapterAnalyzerV2
            print("ğŸ”§ ä½¿ç”¨V2åˆ†æ®µè¾“å‡ºç‰ˆæœ¬")
            chapter_analyzer = ChapterAnalyzerV2(llm, config, intermediate_dir, args.no_time_check)
        else:
            chapter_analyzer = ChapterAnalyzer(llm, config, intermediate_dir, args.no_time_check)
        
        chapter_results = chapter_analyzer.batch_analyze(chapters)
        
        if not chapter_results:
            print("âŒ å•ç« åˆ†æå¤±è´¥ï¼Œé€€å‡º")
            return
        
        # ç¬¬ä¸‰æ­¥ï¼šåˆ†æ®µæ±‡æ€»
        print("\n" + "="*60)
        print("æ­¥éª¤ 3: åˆ†æ®µæ±‡æ€»")
        print("="*60)
        segment_summarizer = SegmentSummarizer(llm, config, intermediate_dir)
        segment_results = segment_summarizer.summarize_segments(chapter_results)
        
        if not segment_results:
            print("âŒ åˆ†æ®µæ±‡æ€»å¤±è´¥ï¼Œé€€å‡º")
            return
        
        # ç¬¬å››æ­¥ï¼šæ•´ä½“åˆ†æ
        print("\n" + "="*60)
        print("æ­¥éª¤ 4: æ•´ä½“åˆ†æ")
        print("="*60)
        global_analyzer = GlobalAnalyzer(llm, config, intermediate_dir)
        global_analysis = global_analyzer.analyze_global(segment_results)
        
        if not global_analysis:
            print("âŒ æ•´ä½“åˆ†æå¤±è´¥ï¼Œé€€å‡º")
            return
        
        # ç¬¬äº”æ­¥ï¼šç”Ÿæˆæ¨¡æ¿
        print("\n" + "="*60)
        print("æ­¥éª¤ 5: ç”Ÿæˆæœ€ç»ˆæ¨¡æ¿")
        print("="*60)
        template_generator = TemplateGenerator(config, args.output)
        success = template_generator.generate_all_templates(global_analysis)
        
        # è®¡ç®—è€—æ—¶
        end_time = datetime.now()
        duration = end_time - start_time
        
        # å®Œæˆ
        print("\n" + "="*60)
        print("âœ… å…¨éƒ¨æµç¨‹å®Œæˆï¼")
        print("="*60)
        print(f"â±ï¸  æ€»è€—æ—¶: {duration}")
        print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {args.output}")
        print(f"\nğŸ“„ æœ€ç»ˆæ¨¡æ¿æ–‡ä»¶ï¼š")
        print(f"   1. world_bible.json        - ä¸–ç•Œè§‚åœ£ç»")
        print(f"   2. plot_framework.json     - æƒ…èŠ‚æ¡†æ¶")
        print(f"   3. writing_guide.json      - å†™ä½œæŒ‡å—")
        print(f"   4. character_templates.json - è§’è‰²æ¨¡æ¿")
        print(f"   5. quality_criteria.json   - è´¨é‡æ ‡å‡†")
        print(f"\nğŸ“‚ ä¸­é—´ç»“æœï¼š")
        print(f"   - å•ç« åˆ†æ: {intermediate_dir}/chapter_summaries/")
        print(f"   - åˆ†æ®µæ±‡æ€»: {intermediate_dir}/segment_summaries/")
        print(f"   - æ•´ä½“åˆ†æ: {intermediate_dir}/global_analysis.json")
        
        if success:
            print(f"\nğŸ‰ æ‰€æœ‰æ¨¡æ¿ç”ŸæˆæˆåŠŸï¼")
        else:
            print(f"\nâš ï¸  éƒ¨åˆ†æ¨¡æ¿ç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥è¾“å‡ºç›®å½•")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
    except Exception as e:
        print(f"\n\nâŒ æ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
