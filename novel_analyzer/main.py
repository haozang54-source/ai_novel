"""
çˆ†æ¬¾åˆ†æå·¥ä½œæµä¸»ç¨‹åº
"""
import os
import sys
import yaml
import argparse
from datetime import datetime
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# å¯¼å…¥LLMç›¸å…³åŒ…
try:
    from langchain_ollama import OllamaLLM
except ImportError:
    from langchain_community.llms import Ollama as OllamaLLM

try:
    from langchain_openai import ChatOpenAI
except ImportError:
    from langchain_community.chat_models import ChatOpenAI

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from analyzers.preprocessor import NovelPreprocessor
from analyzers.chapter_analyzer import ChapterAnalyzer
from analyzers.segment_summarizer import SegmentSummarizer
from analyzers.global_analyzer import GlobalAnalyzer
from analyzers.template_generator import TemplateGenerator


def load_config(config_path: str = None) -> dict:
    """
    åŠ è½½é…ç½®æ–‡ä»¶
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        é…ç½®å­—å…¸
    """
    if config_path is None:
        config_path = os.path.join(os.path.dirname(__file__), 'config', 'config.yaml')
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    return config


def init_llm(config: dict):
    """
    åˆå§‹åŒ–LLMï¼ˆä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ï¼‰
    
    Args:
        config: é…ç½®å­—å…¸
        
    Returns:
        LLMå®ä¾‹
    """
    llm_config = config.get('llm', {})
    
    # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®ï¼ˆä¼˜å…ˆçº§é«˜äºconfig.yamlï¼‰
    provider = os.getenv('LLM_PROVIDER', llm_config.get('provider', 'ollama'))
    temperature = float(os.getenv('LLM_TEMPERATURE', llm_config.get('temperature', 0.3)))
    max_tokens = int(os.getenv('LLM_MAX_TOKENS', llm_config.get('max_tokens', 3000)))
    
    if provider == 'ollama':
        model = os.getenv('OLLAMA_MODEL', llm_config.get('model', 'qwen2.5:7b-instruct'))
        base_url = os.getenv('OLLAMA_BASE_URL', llm_config.get('base_url', 'http://localhost:11434'))
        
        llm = OllamaLLM(
            model=model,
            base_url=base_url,
            temperature=temperature,
        )
        print(f"âœ“ ä½¿ç”¨ Ollama æ¨¡å‹")
        print(f"  æ¨¡å‹: {model}")
        print(f"  åœ°å€: {base_url}")
    
    elif provider == 'openai':
        model = os.getenv('OPENAI_MODEL', llm_config.get('model', 'gpt-3.5-turbo'))
        base_url = os.getenv('OPENAI_API_BASE', llm_config.get('base_url'))
        api_key = os.getenv('OPENAI_API_KEY', llm_config.get('api_key', 'dummy'))
        
        llm = ChatOpenAI(
            model=model,
            base_url=base_url,
            api_key=api_key,
            temperature=temperature,
            max_tokens=max_tokens,
        )
        print(f"âœ“ ä½¿ç”¨ OpenAI å…¼å®¹æ¥å£")
        print(f"  æ¨¡å‹: {model}")
        print(f"  åœ°å€: {base_url}")
        print(f"  æ¸©åº¦: {temperature}, æœ€å¤§Token: {max_tokens}")
    
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„LLM provider: {provider}")
    
    return llm


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='çˆ†æ¬¾å°è¯´åˆ†æå·¥å…·')
    parser.add_argument('--input', '-i', required=True, help='å°è¯´æ–‡ä»¶å¤¹è·¯å¾„')
    parser.add_argument('--output', '-o', required=True, help='è¾“å‡ºæ¨¡æ¿ç›®å½•')
    parser.add_argument('--config', '-c', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    # æ‰“å°æ¬¢è¿ä¿¡æ¯
    print("\n" + "="*60)
    print("ğŸ“š çˆ†æ¬¾å°è¯´åˆ†æå·¥å…·")
    print("="*60 + "\n")
    
    start_time = datetime.now()
    
    # åŠ è½½é…ç½®
    print("âš™ï¸  åŠ è½½é…ç½®...")
    config = load_config(args.config)
    
    # åˆå§‹åŒ–LLM
    print("ğŸ¤– åˆå§‹åŒ–LLM...")
    llm = init_llm(config)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output, exist_ok=True)
    intermediate_dir = os.path.join(args.output, 'intermediate')
    os.makedirs(intermediate_dir, exist_ok=True)
    
    try:
        # ç¬¬ä¸€æ­¥ï¼šé¢„å¤„ç†
        print("\n" + "="*60)
        print("æ­¥éª¤ 1: æ–‡ä»¶é¢„å¤„ç†")
        print("="*60)
        preprocessor = NovelPreprocessor(args.input, config)
        chapters = preprocessor.load_and_process()
        
        if not chapters:
            print("âŒ æ²¡æœ‰å¯å¤„ç†çš„ç« èŠ‚ï¼Œé€€å‡º")
            return
        
        # ç¬¬äºŒæ­¥ï¼šå•ç« åˆ†æ
        print("\n" + "="*60)
        print("æ­¥éª¤ 2: å•ç« åˆ†æ")
        print("="*60)
        chapter_analyzer = ChapterAnalyzer(llm, config, intermediate_dir)
        chapter_results = chapter_analyzer.batch_analyze(chapters)
        
        if not chapter_results:
            print("âŒ å•ç« åˆ†æå¤±è´¥ï¼Œé€€å‡º")
            return
        
        # ç¬¬ä¸‰æ­¥ï¼šåˆ†æ®µæ±‡æ€»
        print("\n" + "="*60)
        print("æ­¥éª¤ 3: åˆ†æ®µæ±‡æ€»")
        print("="*60)
        segment_summarizer = SegmentSummarizer(llm, config, intermediate_dir)
        segment_results = segment_summarizer.summarize_segments(chapter_results)
        
        if not segment_results:
            print("âŒ åˆ†æ®µæ±‡æ€»å¤±è´¥ï¼Œé€€å‡º")
            return
        
        # ç¬¬å››æ­¥ï¼šæ•´ä½“åˆ†æ
        print("\n" + "="*60)
        print("æ­¥éª¤ 4: æ•´ä½“åˆ†æ")
        print("="*60)
        global_analyzer = GlobalAnalyzer(llm, config, intermediate_dir)
        global_analysis = global_analyzer.analyze_global(segment_results)
        
        if not global_analysis:
            print("âŒ æ•´ä½“åˆ†æå¤±è´¥ï¼Œé€€å‡º")
            return
        
        # ç¬¬äº”æ­¥ï¼šç”Ÿæˆæ¨¡æ¿
        print("\n" + "="*60)
        print("æ­¥éª¤ 5: ç”Ÿæˆæœ€ç»ˆæ¨¡æ¿")
        print("="*60)
        template_generator = TemplateGenerator(config, args.output)
        success = template_generator.generate_all_templates(global_analysis)
        
        # è®¡ç®—è€—æ—¶
        end_time = datetime.now()
        duration = end_time - start_time
        
        # å®Œæˆ
        print("\n" + "="*60)
        print("âœ… å…¨éƒ¨æµç¨‹å®Œæˆï¼")
        print("="*60)
        print(f"â±ï¸  æ€»è€—æ—¶: {duration}")
        print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {args.output}")
        print(f"\nğŸ“„ æœ€ç»ˆæ¨¡æ¿æ–‡ä»¶ï¼š")
        print(f"   1. world_bible.json        - ä¸–ç•Œè§‚åœ£ç»")
        print(f"   2. plot_framework.json     - æƒ…èŠ‚æ¡†æ¶")
        print(f"   3. writing_guide.json      - å†™ä½œæŒ‡å—")
        print(f"   4. character_templates.json - è§’è‰²æ¨¡æ¿")
        print(f"   5. quality_criteria.json   - è´¨é‡æ ‡å‡†")
        print(f"\nğŸ“‚ ä¸­é—´ç»“æœï¼š")
        print(f"   - å•ç« åˆ†æ: {intermediate_dir}/chapter_summaries/")
        print(f"   - åˆ†æ®µæ±‡æ€»: {intermediate_dir}/segment_summaries/")
        print(f"   - æ•´ä½“åˆ†æ: {intermediate_dir}/global_analysis.json")
        
        if success:
            print(f"\nğŸ‰ æ‰€æœ‰æ¨¡æ¿ç”ŸæˆæˆåŠŸï¼")
        else:
            print(f"\nâš ï¸  éƒ¨åˆ†æ¨¡æ¿ç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥è¾“å‡ºç›®å½•")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
    except Exception as e:
        print(f"\n\nâŒ æ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
