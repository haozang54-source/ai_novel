"""
ä¿®å¤å·¥å…· - ä½¿ç”¨LLMé‡æ–°ç”Ÿæˆç¼ºå¤±çš„å­—æ®µ
"""
import os
import sys
import json
import time
import argparse
from pathlib import Path
from typing import Dict, List, Optional

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from dotenv import load_dotenv

# å¯¼å…¥LLMç›¸å…³åŒ…
try:
    from langchain_ollama import OllamaLLM
except ImportError:
    from langchain_community.llms import Ollama as OllamaLLM

try:
    from langchain_openai import ChatOpenAI
except ImportError:
    from langchain_community.chat_models import ChatOpenAI

from utils.file_utils import FileUtils
from utils.json_parser import JSONParser


class MissingFieldsRegenerator:
    """ä½¿ç”¨LLMé‡æ–°ç”Ÿæˆç¼ºå¤±å­—æ®µçš„ä¿®å¤å™¨"""
    
    REQUIRED_FIELDS = [
        'characters',
        'locations', 
        'events',
        'world_elements',
        'writing_style_notes',
        'chapter_summary'
    ]
    
    def __init__(self, llm, retry_times: int = 5):
        """
        åˆå§‹åŒ–ä¿®å¤å™¨
        
        Args:
            llm: LangChain LLMå®ä¾‹
            retry_times: æ¯ä¸ªå­—æ®µçš„é‡è¯•æ¬¡æ•°
        """
        self.llm = llm
        self.retry_times = retry_times
    
    def scan_incomplete_chapters(self, summaries_dir: str) -> Dict[int, List[str]]:
        """
        æ‰«æä¸å®Œæ•´çš„ç« èŠ‚
        
        Args:
            summaries_dir: chapter_summariesç›®å½•è·¯å¾„
            
        Returns:
            {chapter_number: [missing_fields]}
        """
        summaries_path = Path(summaries_dir)
        incomplete_chapters = {}
        
        for json_file in sorted(summaries_path.glob("chapter_*.json")):
            try:
                chapter_num = int(json_file.stem.split('_')[1])
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                missing_fields = [
                    field for field in self.REQUIRED_FIELDS 
                    if field not in data
                ]
                
                if missing_fields:
                    incomplete_chapters[chapter_num] = missing_fields
                    
            except Exception as e:
                print(f"âš ï¸  è¯»å– {json_file.name} å¤±è´¥: {e}")
        
        return incomplete_chapters
    
    def load_chapter_content(self, chapter_num: int, novel_dir: str) -> Optional[str]:
        """
        åŠ è½½ç« èŠ‚åŸå§‹å†…å®¹
        
        Args:
            chapter_num: ç« èŠ‚ç¼–å·
            novel_dir: å°è¯´æ–‡ä»¶å¤¹è·¯å¾„
            
        Returns:
            ç« èŠ‚å†…å®¹æ–‡æœ¬
        """
        novel_path = Path(novel_dir)
        
        # å°è¯•å¸¸è§çš„æ–‡ä»¶åæ ¼å¼
        patterns = [
            f"ç¬¬{chapter_num}ç« *.txt",
            f"ç¬¬{chapter_num:03d}ç« *.txt",
            f"chapter_{chapter_num:03d}.txt",
            f"{chapter_num:03d}*.txt"
        ]
        
        for pattern in patterns:
            files = list(novel_path.glob(pattern))
            if files:
                try:
                    with open(files[0], 'r', encoding='utf-8') as f:
                        content = f.read().strip()
                    
                    # æ™ºèƒ½æˆªæ–­
                    max_length = 6000
                    if len(content) > max_length:
                        truncate_pos = max_length
                        for i in range(max_length, max(0, max_length - 200), -1):
                            if content[i] in 'ã€‚ï¼ï¼Ÿâ€¦\n':
                                truncate_pos = i + 1
                                break
                        content = content[:truncate_pos]
                    
                    return content
                except Exception as e:
                    print(f"      âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        
        return None
    
    def regenerate_field(self, field_name: str, content: str, chapter_num: int) -> Optional[any]:
        """
        ä½¿ç”¨LLMé‡æ–°ç”Ÿæˆå•ä¸ªå­—æ®µ
        
        Args:
            field_name: å­—æ®µåç§°
            content: ç« èŠ‚å†…å®¹
            chapter_num: ç« èŠ‚ç¼–å·
            
        Returns:
            ç”Ÿæˆçš„å­—æ®µæ•°æ®
        """
        for attempt in range(self.retry_times):
            try:
                if field_name == 'characters':
                    result = self._extract_characters(content)
                elif field_name == 'locations':
                    result = self._extract_locations(content)
                elif field_name == 'events':
                    result = self._extract_events(content)
                elif field_name == 'world_elements':
                    result = self._extract_world_elements(content)
                elif field_name == 'writing_style_notes':
                    result = self._extract_writing_style(content)
                elif field_name == 'chapter_summary':
                    result = self._extract_chapter_summary(content)
                else:
                    return None
                
                if result is not None:
                    return result
                    
            except Exception as e:
                if attempt < self.retry_times - 1:
                    print(f"        âš ï¸  é‡è¯• {attempt + 1}/{self.retry_times}: {e}")
                    time.sleep(2)
                else:
                    print(f"        âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")
        
        return None
    
    def _extract_characters(self, content: str) -> Optional[List]:
        """æå–è§’è‰²ä¿¡æ¯"""
        prompt = f"""åˆ†æä»¥ä¸‹ç« èŠ‚å†…å®¹ï¼Œåªæå–è§’è‰²ä¿¡æ¯ã€‚

ç« èŠ‚å†…å®¹ï¼š
{content}

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºè§’è‰²åˆ—è¡¨ï¼Œä¸è¦æ·»åŠ å…¶ä»–æ–‡å­—ï¼š
[
  {{
    "name": "è§’è‰²å",
    "role": "protagonist/antagonist/supporting",
    "first_appearance": true,
    "status_changes": ["å˜åŒ–æè¿°"],
    "relationships": [
      {{
        "target": "ç›¸å…³è§’è‰²å",
        "relation_type": "ä¸ˆå¤«/å¦»å­/çˆ¶äº²/æ¯äº²/å…„å¼Ÿ/å§å¦¹/å¸ˆå¾’/æœ‹å‹/æ•Œäºº/æ‹äººç­‰",
        "description": "å…³ç³»æè¿°"
      }}
    ],
    "appearance_traits": ["å¤–è²Œç‰¹å¾"],
    "personality_traits": ["æ€§æ ¼ç‰¹å¾"]
  }}
]

åªè¾“å‡ºJSONæ•°ç»„ï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚"""
        
        response = self.llm.invoke(prompt)
        response_text = response.content if hasattr(response, 'content') else str(response)
        return JSONParser.parse(response_text)
    
    def _extract_locations(self, content: str) -> Optional[List]:
        """æå–åœ°ç‚¹ä¿¡æ¯"""
        prompt = f"""åˆ†æä»¥ä¸‹ç« èŠ‚å†…å®¹ï¼Œåªæå–åœ°ç‚¹ä¿¡æ¯ã€‚

ç« èŠ‚å†…å®¹ï¼š
{content}

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºåœ°ç‚¹åˆ—è¡¨ï¼š
[
  {{
    "name": "åœ°ç‚¹åç§°",
    "type": "åŸå¸‚/æ‘è½/å±±è„‰/å®—é—¨/ç§˜å¢ƒ/å…¶ä»–",
    "description": "åœ°ç‚¹æè¿°",
    "importance": "high/medium/low"
  }}
]

åªè¾“å‡ºJSONæ•°ç»„ã€‚"""
        
        response = self.llm.invoke(prompt)
        response_text = response.content if hasattr(response, 'content') else str(response)
        return JSONParser.parse(response_text)
    
    def _extract_events(self, content: str) -> Optional[List]:
        """æå–äº‹ä»¶ä¿¡æ¯"""
        prompt = f"""åˆ†æä»¥ä¸‹ç« èŠ‚å†…å®¹ï¼Œåªæå–å…³é”®äº‹ä»¶ã€‚

ç« èŠ‚å†…å®¹ï¼š
{content}

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºäº‹ä»¶åˆ—è¡¨ï¼š
[
  {{
    "event_type": "æˆ˜æ–—/ä¿®ç‚¼/æ¢ç´¢/ç¤¾äº¤/é˜´è°‹/å…¶ä»–",
    "description": "äº‹ä»¶æè¿°",
    "participants": ["å‚ä¸è€…1", "å‚ä¸è€…2"],
    "location": "å‘ç”Ÿåœ°ç‚¹",
    "outcome": "äº‹ä»¶ç»“æœ"
  }}
]

åªè¾“å‡ºJSONæ•°ç»„ã€‚"""
        
        response = self.llm.invoke(prompt)
        response_text = response.content if hasattr(response, 'content') else str(response)
        return JSONParser.parse(response_text)
    
    def _extract_world_elements(self, content: str) -> Optional[Dict]:
        """æå–ä¸–ç•Œè§‚å…ƒç´ """
        prompt = f"""åˆ†æä»¥ä¸‹ç« èŠ‚å†…å®¹ï¼Œæå–ä¸–ç•Œè§‚å…ƒç´ ã€‚

ç« èŠ‚å†…å®¹ï¼š
{content}

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼š
{{
  "cultivation_system": ["ä¿®ç‚¼ä½“ç³»ç›¸å…³"],
  "magic_items": ["æ³•å®ã€çµè¯ç­‰"],
  "organizations": ["é—¨æ´¾ã€åŠ¿åŠ›ç­‰"],
  "rules_laws": ["ä¸–ç•Œè§„åˆ™ã€å¤©é“ç­‰"],
  "other": ["å…¶ä»–ä¸–ç•Œè§‚å…ƒç´ "]
}}

åªè¾“å‡ºJSONå¯¹è±¡ã€‚"""
        
        response = self.llm.invoke(prompt)
        response_text = response.content if hasattr(response, 'content') else str(response)
        return JSONParser.parse(response_text)
    
    def _extract_writing_style(self, content: str) -> Optional[Dict]:
        """æå–å†™ä½œé£æ ¼"""
        prompt = f"""åˆ†æä»¥ä¸‹ç« èŠ‚çš„å†™ä½œé£æ ¼ã€‚

ç« èŠ‚å†…å®¹ï¼š
{content}

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼š
{{
  "narrative_techniques": ["å™äº‹æŠ€å·§"],
  "language_features": ["è¯­è¨€ç‰¹ç‚¹"],
  "pacing_notes": "èŠ‚å¥æ§åˆ¶è¯´æ˜",
  "emotional_tone": "æƒ…æ„ŸåŸºè°ƒ",
  "notable_phrases": ["é‡‘å¥ã€ç‰¹è‰²è¡¨è¾¾"]
}}

åªè¾“å‡ºJSONå¯¹è±¡ã€‚"""
        
        response = self.llm.invoke(prompt)
        response_text = response.content if hasattr(response, 'content') else str(response)
        return JSONParser.parse(response_text)
    
    def _extract_chapter_summary(self, content: str) -> Optional[str]:
        """æå–ç« èŠ‚æ‘˜è¦"""
        prompt = f"""ç”¨1-2å¥è¯æ¦‚æ‹¬ä»¥ä¸‹ç« èŠ‚çš„æ ¸å¿ƒå†…å®¹ã€‚

ç« èŠ‚å†…å®¹ï¼š
{content}

åªè¾“å‡ºæ¦‚æ‹¬æ–‡å­—ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""
        
        response = self.llm.invoke(prompt)
        return response.content if hasattr(response, 'content') else str(response)
    
    def repair_chapter(
        self, 
        chapter_num: int, 
        missing_fields: List[str],
        summaries_dir: str,
        novel_dir: str
    ) -> bool:
        """
        ä¿®å¤å•ä¸ªç« èŠ‚çš„ç¼ºå¤±å­—æ®µ
        
        Args:
            chapter_num: ç« èŠ‚ç¼–å·
            missing_fields: ç¼ºå¤±çš„å­—æ®µåˆ—è¡¨
            summaries_dir: chapter_summariesç›®å½•
            novel_dir: å°è¯´åŸå§‹æ–‡ä»¶ç›®å½•
            
        Returns:
            æ˜¯å¦ä¿®å¤æˆåŠŸ
        """
        print(f"\nğŸ“„ ä¿®å¤ç« èŠ‚ {chapter_num}")
        print(f"   ç¼ºå¤±å­—æ®µ: {', '.join(missing_fields)}")
        
        # åŠ è½½ç« èŠ‚å†…å®¹
        print(f"   â³ åŠ è½½ç« èŠ‚å†…å®¹...")
        content = self.load_chapter_content(chapter_num, novel_dir)
        
        if not content:
            print(f"   âŒ æ— æ³•æ‰¾åˆ°ç« èŠ‚ {chapter_num} çš„åŸå§‹æ–‡ä»¶")
            return False
        
        # åŠ è½½ç°æœ‰æ•°æ®
        summary_file = Path(summaries_dir) / f"chapter_{chapter_num:03d}.json"
        try:
            with open(summary_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except Exception as e:
            print(f"   âŒ è¯»å–ç°æœ‰æ•°æ®å¤±è´¥: {e}")
            return False
        
        # é€ä¸ªé‡æ–°ç”Ÿæˆç¼ºå¤±å­—æ®µ
        success_count = 0
        for field in missing_fields:
            print(f"   â†’ ç”Ÿæˆ {field}...")
            
            result = self.regenerate_field(field, content, chapter_num)
            
            if result is not None:
                data[field] = result
                print(f"      âœ“ æˆåŠŸ")
                success_count += 1
            else:
                print(f"      âœ— å¤±è´¥")
            
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        # ä¿å­˜æ›´æ–°åçš„æ•°æ®
        if success_count > 0:
            try:
                with open(summary_file, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                print(f"   âœ… æˆåŠŸä¿®å¤ {success_count}/{len(missing_fields)} ä¸ªå­—æ®µ")
                return success_count == len(missing_fields)
            except Exception as e:
                print(f"   âŒ ä¿å­˜å¤±è´¥: {e}")
                return False
        else:
            print(f"   âŒ æ‰€æœ‰å­—æ®µé‡æ–°ç”Ÿæˆå‡å¤±è´¥")
            return False


def init_llm(config: dict):
    """
    åˆå§‹åŒ–LLMï¼ˆä»main.pyå¤åˆ¶ï¼‰
    
    Args:
        config: é…ç½®å­—å…¸
        
    Returns:
        LLMå®ä¾‹
    """
    llm_config = config.get('llm', {})
    
    # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®ï¼ˆä¼˜å…ˆçº§é«˜äºconfig.yamlï¼‰
    provider = os.getenv('LLM_PROVIDER', llm_config.get('provider', 'ollama'))
    temperature = float(os.getenv('LLM_TEMPERATURE', llm_config.get('temperature', 0.3)))
    max_tokens = int(os.getenv('LLM_MAX_TOKENS', llm_config.get('max_tokens', 3000)))
    
    if provider == 'ollama':
        model = os.getenv('OLLAMA_MODEL', llm_config.get('model', 'qwen2.5:7b-instruct'))
        base_url = os.getenv('OLLAMA_BASE_URL', llm_config.get('base_url', 'http://localhost:11434'))
        
        llm = OllamaLLM(
            model=model,
            base_url=base_url,
            temperature=temperature,
        )
        print(f"âœ“ ä½¿ç”¨ Ollama æ¨¡å‹: {model}")
    
    elif provider == 'openai':
        model = os.getenv('OPENAI_MODEL', llm_config.get('model', 'gpt-3.5-turbo'))
        base_url = os.getenv('OPENAI_API_BASE', llm_config.get('base_url'))
        api_key = os.getenv('OPENAI_API_KEY', llm_config.get('api_key', 'dummy'))
        
        llm = ChatOpenAI(
            model=model,
            base_url=base_url,
            api_key=api_key,
            temperature=temperature,
            max_tokens=max_tokens,
        )
        print(f"âœ“ ä½¿ç”¨ OpenAI å…¼å®¹æ¥å£: {model}")
    
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„LLM provider: {provider}")
    
    return llm


def main():
    parser = argparse.ArgumentParser(description='ä½¿ç”¨LLMé‡æ–°ç”Ÿæˆç¼ºå¤±å­—æ®µ')
    parser.add_argument('--summaries-dir', required=True, help='chapter_summariesç›®å½•è·¯å¾„')
    parser.add_argument('--novel-dir', required=True, help='å°è¯´åŸå§‹æ–‡ä»¶ç›®å½•è·¯å¾„')
    parser.add_argument('--config', help='é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--report-only', action='store_true', help='åªç”ŸæˆæŠ¥å‘Šï¼Œä¸æ‰§è¡Œä¿®å¤')
    parser.add_argument('--auto-confirm', action='store_true', help='è‡ªåŠ¨ç¡®è®¤ï¼Œä¸è¯¢é—®')
    
    args = parser.parse_args()
    
    # åŠ è½½ç¯å¢ƒå˜é‡ï¼ˆä»é¡¹ç›®æ ¹ç›®å½•ï¼‰
    env_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')
    if os.path.exists(env_path):
        load_dotenv(env_path)
    else:
        load_dotenv()  # å°è¯•ä»å½“å‰ç›®å½•åŠ è½½
    
    # åŠ è½½é…ç½®
    if args.config:
        import yaml
        with open(args.config, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
    else:
        import yaml
        default_config_path = os.path.join(
            os.path.dirname(os.path.dirname(__file__)), 
            'config', 
            'config.yaml'
        )
        with open(default_config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
    
    # åˆå§‹åŒ–LLM
    llm = init_llm(config)
    
    # åˆ›å»ºä¿®å¤å™¨
    regenerator = MissingFieldsRegenerator(llm)
    
    # æ‰«æä¸å®Œæ•´ç« èŠ‚
    print("ğŸ” æ‰«æä¸å®Œæ•´ç« èŠ‚...\n")
    incomplete_chapters = regenerator.scan_incomplete_chapters(args.summaries_dir)
    
    if not incomplete_chapters:
        print("âœ… æ‰€æœ‰ç« èŠ‚æ•°æ®å®Œæ•´ï¼")
        return
    
    # æ˜¾ç¤ºæŠ¥å‘Š
    print("=" * 80)
    print(f"ğŸ“Š å‘ç° {len(incomplete_chapters)} ä¸ªä¸å®Œæ•´ç« èŠ‚\n")
    
    for chapter_num in sorted(incomplete_chapters.keys()):
        missing_fields = incomplete_chapters[chapter_num]
        print(f"  ğŸ“„ ç« èŠ‚ {chapter_num:03d}")
        print(f"     ç¼ºå¤±å­—æ®µ ({len(missing_fields)}): {', '.join(missing_fields)}")
    
    print("=" * 80)
    
    # å¦‚æœåªæ˜¯ç”ŸæˆæŠ¥å‘Š
    if args.report_only:
        report_file = os.path.join(args.summaries_dir, 'incomplete_chapters_report.json')
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump({
                'total_incomplete': len(incomplete_chapters),
                'chapters': {
                    str(ch): fields for ch, fields in incomplete_chapters.items()
                }
            }, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ“ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        return
    
    # ç¡®è®¤ä¿®å¤
    if not args.auto_confirm:
        response = input(f"\næ˜¯å¦å¼€å§‹ä½¿ç”¨LLMé‡æ–°ç”Ÿæˆè¿™ {len(incomplete_chapters)} ä¸ªç« èŠ‚çš„ç¼ºå¤±å­—æ®µï¼Ÿ(y/n): ")
        if response.lower() != 'y':
            print("âŒ å·²å–æ¶ˆ")
            return
    
    # æ‰§è¡Œä¿®å¤
    print("\n" + "=" * 80)
    print("ğŸ”§ å¼€å§‹ä¿®å¤...\n")
    
    success_count = 0
    failed_chapters = []
    
    for chapter_num in sorted(incomplete_chapters.keys()):
        missing_fields = incomplete_chapters[chapter_num]
        
        success = regenerator.repair_chapter(
            chapter_num,
            missing_fields,
            args.summaries_dir,
            args.novel_dir
        )
        
        if success:
            success_count += 1
        else:
            failed_chapters.append(chapter_num)
    
    # ä¿®å¤æ€»ç»“
    print("\n" + "=" * 80)
    print("ğŸ“Š ä¿®å¤å®Œæˆï¼\n")
    print(f"  âœ… æˆåŠŸä¿®å¤: {success_count}/{len(incomplete_chapters)} ä¸ªç« èŠ‚")
    
    if failed_chapters:
        print(f"  âŒ ä¿®å¤å¤±è´¥: {len(failed_chapters)} ä¸ªç« èŠ‚")
        print(f"     ç« èŠ‚ç¼–å·: {failed_chapters}")
    
    print("=" * 80)


if __name__ == '__main__':
    main()
